This is a work in progress and will eventually be converted to a more readable
format.

TraceR is a replay tool targeted to simulate control flow of application on
prototype systems, i.e., if control flow of an application, which includes
expected computation tasks, communication routines, and their dependencies, is
provided to TraceR, it will mimic the flow on a hypothetical system with a given
compute and communication capability. As of now, the control flow is captured by
ether emulating applications using BigSim or by linking with ScoreP. CODES 
is used for simulating the communication on the network.

Expected work flow:

1) Write an MPI application. (Avoid global variables so that the application be
run with virtualization if using BigSim).

If using BigSim follows steps 2-4, else follow step 5.
2) Compile BigSim/Charm++ for emulation. Use any one of the following commands:

- To use UDP as BigSim/Charm++'s communication layer:
  ./build bgampi net-linux-x86_64 bigemulator --with-production --enable-tracing
  ./build bgampi net-darwin-x86_64 bigemulator --with-production --enable-tracing

  or explicitly provide the compiler optimization level
  ./build bgampi net-linux-x86_64 bigemulator -O2

- To use MPI as BigSim/Charm++'s communication layer:
  ./build bgampi mpi-linux-x86_64 bigemulator --with-production --enable-tracing

Note that this build is used to compile MPI applications so that traces can be
generated. Hence, the communication layer used by BigSim/Charm++ is not
important.  During simulation, the communication will be replayed using the
network simulator from CODES. However, the computation time captured here can be
important if it is not being explicitly replaced at simulation time using
configuration options. So using appropriate compiler flags is important.

3) Compile the MPI application from Step 1 using BigSim/Charm++ from Step 2.

Example commands:
$CHARM_DIR/bin/ampicc -O2 simplePrg.c -o simplePrg_c
$CHARM_DIR/bin/ampiCC -O2 simplePrg.cc -o simplePrg_cxx

4) Emulation to generate traces. When the binary generated in Step 3 is run,
BigSim/Charm++ runs the program on the allocated cores as if it would run in the
usual case. Users should provide a few additional arguments to specify the
number of MPI processes in the prototype systems.

If using UDP as the BigSim/Charm++'s communication layer:
./charmrun +p<number of real processes> ++nodelist <machine file> ./pgm <program arguments> +vp<number of processes expected on the future system> +x<x dim> +y<y dim> +z<z dim> +bglog

If using MPI as the BigSim/Charm++'s communication layer:
mpirun -n<number of real processes> ./pgm <program arguments> +vp<number of processes expected on the future system> +x<x dim> +y<y dim> +z<z dim> +bglog

Number of real processes is typically equalt to the number cores the emulation
is being run on.

machine file is the list of systems the emulation should be run on (similar to
machine file for MPI; refer to Charm++ website for more details).

vp is the number of MPI ranks that are to be emulated. For simple tests, it can
be same as the number of real processes, in which case one MPI rank is run on
each real processes (as it happens when a regular program is run). When the
number of vp (virtual processes) is higher, BigSim launches user level threads
to execute multiple MPI ranks with a process.

+x +y +z defines a 3D grid of the virtual processes. The product of these three
dimensions must match the number of vp's. These arguments do not have any
effect on the emulation, but exist due to historical reasons.

+bglog instructs bigsim to write the logs to files.

When this run finished, you should use many files named bgTrace* in the
directory. The total number of such files equals the number of real processes
plus one. Their names are bgTrace, bgTrace0, bgTrace1, so on.

Create a new folder and move all bgTrace to that folder.

5) Following instructions in README.OTF to generate OTF2 traces.

6) Simulation. To run simulation, 2 files are needed: a tracer config file, 
and a codes config file. Optionally, mapping files can also be provided.

Tracer config file: sample found at tracer/jacobi2d/tracer_config (BigSim) or tracer/stencil4d-otf/tracer_config (OTF) Format (expected content on each line of the file):
<global_map file>
<num jobs>
<Trace path for job0> <map file for job0> <number of ranks in job0> <iterations (use 1 if running in normal mode)>
<Trace path for job1> <map file for job1> <number of ranks in job1> <iterations (use 1 if running in normal mode)>
...
```
If <global_map file> is not needed, use NA for it and <map file for job*>.
For generating simple global and job map file, use the code in utils.

CODES config files: samples at tracer/conf/tracer-torus.conf,
tracer/conf/tracer-dragonfly.conf, tracer/conf/tracer-fattree-type2.conf
(only these three networks have been improved and tested). Additional ReadMe on
format of the CODES config file can be found in the CODES repository. Brief
summary follows:

LPGROUPS, MODELNET_GRP, PARAMS are keywords and should be used as in.  Each
repetition in the MODELNET_GRP describes a router and its associated component.
Number of servers is the number of MPI processes/cores, while the modelnet_* is
the number of NICs. For torus, this value has to be 1; for dragonfly, it should
be router radix divided by 4; for the fat-tree, it should be router radix
divided by 2.  For the dragonfly network, dragonfly_router should also be
specified (as 1).  Similarly, the fat-tree config file requires specifying
fattree_switch which can be 2 or 3, depending on the number of levels in the
fat-tree. Note that the total number of cores specified in the CODES config file
can be greater than the number of MPI processes being simulated (specified in
the tracer config file).

Other common parameters:
packet_size/chunk_size (both should have the same value): size of the packets
created by NIC for transmission on the network. Smaller the packet size, longer
the time for which simulation will run (in real time). Larger the packet size,
the less accurate the predictions are expected to be (in virtual time). Packet
sizes of 512 bytes to 4096 bytes are commonly used.

modelnet_order = torus/dragonfly/fattree

modelnet_scheduler =
fcfs : packetize messages one by one.
round-robin : packetize message in a round robin manner.

message_size = PDES parameter (keep constant at 512)

router_delay = delay at each router for packet transmission (in nano seconds)

soft_delay = delay caused by software stack such as that of MPI (in nano
seconds)

link_bandwidth = bandwidth of each link in the system (in GB/s)

cn_bandwidth = bandwidth of connection between NIC and router (in GB/s)

buffer_size/vc_size = size of channels used to store transient packets at routers (in
bytes). Typical value is 64*packet_size.

routing = how are packets being routed. Options depend on the network:
torus = static/adaptive
dragonfly = minimal/nonminimal/adaptive
fat-tree = adaptive by default (no other option available)

Network specific parameters:

Torus: n_dims - number of dimensions in the torus
dim_length - length of each dimension

Dragonfly: num_routers - number of routers within a group.
global_bandwidth - bandwidth of the links that connect groups.

Fat-tree: ft_type - always choose 1
num_levels - number of levels in the fat-tree (2 or 3)
switch_radix -  radix of the switch being used
switch_count - number of switches at leaf level.

Publications that describe implementation of TraceR in detail:
Nikhil Jain, Abhinav Bhatele, Sam White, Todd Gamblin, and Laxmikant Kale.
Evaluating HPC Networks via Simulation of Parallel Workloads. SC 2016.

Bilge Acun, Nikhil Jain, Abhinav Bhatele, Misbah Mubarak, Christopher Carothers,
Laxmikant Kale. Preliminary Evaluation of a Parallel Trace Replay Tool for HPC
Network Simulations. Workshop on Parallel and Distributed Agent-Based
Simulations at EURO-PAR 2015.

More details can be found in Chapter 5 of this thesis:
http://charm.cs.illinois.edu/newPapers/16-02/Jain_Thesis.pdf
